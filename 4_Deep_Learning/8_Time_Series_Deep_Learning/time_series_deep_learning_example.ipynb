{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Deep Learning Example\n",
    "\n",
    "This notebook demonstrates a complete workflow for time series forecasting using deep learning models, covering data generation, Exploratory Data Analysis (EDA), data preprocessing, model definition, training, hyperparameter tuning with Optuna, and model saving/loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation (Synthetic Time Series)\n",
    "\n",
    "We'll create a synthetic time series with a trend, seasonality, and noise for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(num_points=1000):\n",
    "    time = np.arange(num_points)\n",
    "    # Trend\n",
    "    trend = 0.02 * time\n",
    "    # Seasonality\n",
    "    seasonality = 10 * np.sin(time / 20) + 5 * np.cos(time / 50)\n",
    "    # Noise\n",
    "    noise = np.random.normal(0, 1, num_points)\n",
    "    \n",
    "    data = trend + seasonality + noise\n",
    "    \n",
    "    df = pd.DataFrame({'time': time, 'value': data})\n",
    "    return df\n",
    "\n",
    "df = generate_time_series()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Visualize the time series, check for stationarity, and observe patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df['time'], df['value'])\n",
    "plt.title('Synthetic Time Series')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Rolling statistics to check for stationarity (visual inspection)\n",
    "rolling_mean = df['value'].rolling(window=50).mean()\n",
    "rolling_std = df['value'].rolling(window=50).std()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df['time'], df['value'], label='Original')\n",
    "plt.plot(df['time'], rolling_mean, label='Rolling Mean')\n",
    "plt.plot(df['time'], rolling_std, label='Rolling Std')\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing for Deep Learning\n",
    "\n",
    "We need to transform the time series into sequences (input features) and corresponding target values for supervised learning. This involves creating lagged features and splitting the data into training, validation, and test sets while preserving the temporal order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        y = data[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 50 # Number of past time steps to consider\n",
    "X, y = create_sequences(df['value'].values, sequence_length)\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "# Train-Validation-Test Split (Time Series Specific)\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.15)\n",
    "test_size = len(X) - train_size - val_size\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
    "X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float().unsqueeze(-1) # Add feature dimension\n",
    "y_train_tensor = torch.from_numpy(y_train).float().unsqueeze(-1)\n",
    "X_val_tensor = torch.from_numpy(X_val).float().unsqueeze(-1)\n",
    "y_val_tensor = torch.from_numpy(y_val).float().unsqueeze(-1)\n",
    "X_test_tensor = torch.from_numpy(X_test).float().unsqueeze(-1)\n",
    "y_test_tensor = torch.from_numpy(y_test).float().unsqueeze(-1)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64 # This will be tuned by Optuna later\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) # No shuffle for time series\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Learning Model Definition (LSTM)\n",
    "\n",
    "We'll define a simple LSTM model for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :]) # Take the output from the last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training and Validation Function\n",
    "\n",
    "A function to train and evaluate the model, which will be used by Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, n_epochs, trial=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
    "\n",
    "        if trial:\n",
    "            trial.report(epoch_val_loss, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "                \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning with Optuna\n",
    "\n",
    "We'll use Optuna to find the best hyperparameters for our LSTM model. The objective function will train the model and return the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [32, 64, 128])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    n_epochs = trial.suggest_int('n_epochs', 10, 50)\n",
    "\n",
    "    # Update DataLoader with current batch_size\n",
    "    train_loader_optuna = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_loader_optuna = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = LSTMModel(input_size=1, hidden_size=hidden_size, num_layers=num_layers, output_size=1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    _, _, val_losses = train_model(model, train_loader_optuna, val_loader_optuna, optimizer, criterion, n_epochs, trial)\n",
    "    \n",
    "    return val_losses[-1] # Return the final validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study and optimize\n",
    "study_name = \"time_series_lstm_optimization\"\n",
    "storage_name = \"sqlite:///\" + study_name + \".db\"\n",
    "\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    print(\"Loaded existing study.\")\n",
    "except KeyError:\n",
    "    study = optuna.create_study(direction='minimize', study_name=study_name, storage=storage_name)\n",
    "    print(\"Created a new study.\")\n",
    "\n",
    "print(\"Starting Optuna optimization...\")\n",
    "study.optimize(objective, n_trials=20, timeout=600) # Run 20 trials or for 600 seconds\n",
    "\n",
    "print(\"Optimization finished.\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"  Value (Validation Loss): {best_trial.value:.4f}\")\n",
    "print(f\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna Visualization (Requires `plotly` and `matplotlib`)\n",
    "\n",
    "You can visualize the optimization process and results using Optuna's built-in plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fig_history = optuna.visualization.plot_optimization_history(study)\n",
    "    fig_history.show()\n",
    "    fig_history.write_image(\"optuna_optimization_history.png\")\n",
    "    print(\"Optimization history plot saved to optuna_optimization_history.png\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate optimization history plot: {e}\")\n",
    "\n",
    "try:\n",
    "    fig_importance = optuna.visualization.plot_param_importances(study)\n",
    "    fig_importance.show()\n",
    "    fig_importance.write_image(\"optuna_param_importances.png\")\n",
    "    print(\"Parameter importances plot saved to optuna_param_importances.png\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate parameter importances plot: {e}\")\n",
    "\n",
    "try:\n",
    "    fig_slice = optuna.visualization.plot_slice(study)\n",
    "    fig_slice.show()\n",
    "    fig_slice.write_image(\"optuna_slice_plot.png\")\n",
    "    print(\"Slice plot saved to optuna_slice_plot.png\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate slice plot: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Final Model with Best Hyperparameters\n",
    "\n",
    "After finding the best hyperparameters, we train the model on the combined training and validation data (or just training data, depending on strategy) and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best_trial.params\n",
    "\n",
    "final_model = LSTMModel(input_size=1,\n",
    "                        hidden_size=best_params['hidden_size'],\n",
    "                        num_layers=best_params['num_layers'],\n",
    "                        output_size=1)\n",
    "\n",
    "final_optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "final_criterion = nn.MSELoss()\n",
    "\n",
    "# Combine train and validation data for final training\n",
    "X_train_val_tensor = torch.cat((X_train_tensor, X_val_tensor), dim=0)\n",
    "y_train_val_tensor = torch.cat((y_train_tensor, y_val_tensor), dim=0)\n",
    "\n",
    "train_val_dataset = TensorDataset(X_train_val_tensor, y_train_val_tensor)\n",
    "train_val_loader = DataLoader(train_val_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "print(\"Training final model with best hyperparameters...\")\n",
    "final_model, final_train_losses, final_val_losses = train_model(final_model, train_val_loader, val_loader, final_optimizer, final_criterion, best_params['n_epochs'])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(final_train_losses, label='Final Train Loss')\n",
    "plt.plot(final_val_losses, label='Final Validation Loss')\n",
    "plt.title('Final Model Training & Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation on Test Set\n",
    "\n",
    "Evaluate the trained model on the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "final_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = final_model(inputs)\n",
    "        loss = final_criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        predictions.extend(outputs.cpu().numpy().flatten())\n",
    "        true_values.extend(targets.cpu().numpy().flatten())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(true_values, label='True Values')\n",
    "plt.plot(predictions, label='Predictions')\n",
    "plt.title('Test Set Predictions vs True Values')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Saving and Loading\n",
    "\n",
    "Save the trained model's state dictionary and demonstrate how to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'best_time_series_model.pth'\n",
    "torch.save(final_model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Demonstrate loading the model\n",
    "loaded_model = LSTMModel(input_size=1,\n",
    "                         hidden_size=best_params['hidden_size'],\n",
    "                         num_layers=best_params['num_layers'],\n",
    "                         output_size=1)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path))\n",
    "loaded_model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# You can now use loaded_model for inference\n",
    "# Example: Make a prediction with the loaded model\n",
    "sample_input = X_test_tensor[0:1].to(device)\n",
    "loaded_prediction = loaded_model(sample_input).item()\n",
    "print(f\"Sample input: {sample_input.cpu().numpy().flatten()}\")\n",
    "print(f\"Prediction from loaded model for sample input: {loaded_prediction:.4f}\")\n",
    "print(f\"True value for sample input: {y_test_tensor[0].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Further Considerations\n",
    "\n",
    "-   **Feature Scaling**: For real-world data, it's crucial to scale your time series data (e.g., using `MinMaxScaler` or `StandardScaler`) before feeding it to neural networks.\n",
    "-   **More Complex Models**: Explore more advanced architectures like GRUs, Transformers, or even CNNs for time series.\n",
    "-   **Multivariate Time Series**: Extend the model to handle multiple input features.\n",
    "-   **Forecasting Horizon**: Adapt the model to predict multiple future time steps.\n",
    "-   **Error Metrics**: Use appropriate time series error metrics like MAE, RMSE, MAPE.\n",
    "-   **Cross-Validation**: For more robust evaluation, consider time series cross-validation strategies (e.g., rolling origin cross-validation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
